{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c2b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly==5.22.0 in ./anaconda3/lib/python3.11/site-packages (5.22.0)\n",
      "Requirement already satisfied: regex==2024.5.15 in ./anaconda3/lib/python3.11/site-packages (2024.5.15)\n",
      "Requirement already satisfied: requests==2.31.0 in ./anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in ./anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: torch==2.3.0 in ./anaconda3/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: tqdm==4.66.4 in ./anaconda3/lib/python3.11/site-packages (4.66.4)\n",
      "Requirement already satisfied: transformers==4.41.0 in ./anaconda3/lib/python3.11/site-packages (4.41.0)\n",
      "Requirement already satisfied: umap==0.1.1 in ./anaconda3/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: umap-learn==0.5.6 in ./anaconda3/lib/python3.11/site-packages (0.5.6)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./anaconda3/lib/python3.11/site-packages (from plotly==5.22.0) (8.2.2)\n",
      "Requirement already satisfied: packaging in ./anaconda3/lib/python3.11/site-packages (from plotly==5.22.0) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests==2.31.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests==2.31.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests==2.31.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests==2.31.0) (2023.7.22)\n",
      "Requirement already satisfied: torchvision in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (0.18.0)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (1.3.2)\n",
      "Requirement already satisfied: scipy in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (1.11.1)\n",
      "Requirement already satisfied: nltk in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (0.23.4)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.11/site-packages (from torch==2.3.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./anaconda3/lib/python3.11/site-packages (from torch==2.3.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.11/site-packages (from torch==2.3.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.11/site-packages (from torch==2.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.11/site-packages (from torch==2.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./anaconda3/lib/python3.11/site-packages (from torch==2.3.0) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.11/site-packages (from transformers==4.41.0) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./anaconda3/lib/python3.11/site-packages (from transformers==4.41.0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./anaconda3/lib/python3.11/site-packages (from transformers==4.41.0) (0.4.3)\n",
      "Requirement already satisfied: numba>=0.51.2 in ./anaconda3/lib/python3.11/site-packages (from umap-learn==0.5.6) (0.57.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in ./anaconda3/lib/python3.11/site-packages (from umap-learn==0.5.6) (0.5.13)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in ./anaconda3/lib/python3.11/site-packages (from numba>=0.51.2->umap-learn==0.5.6) (0.40.0)\n",
      "Requirement already satisfied: joblib>=0.11 in ./anaconda3/lib/python3.11/site-packages (from pynndescent>=0.5->umap-learn==0.5.6) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers==2.2.2) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.3.0) (2.1.1)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.11/site-packages (from nltk->sentence-transformers==2.2.2) (8.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.11/site-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./anaconda3/lib/python3.11/site-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly==5.22.0 regex==2024.5.15 requests==2.31.0 sentence-transformers==2.2.2 torch==2.3.0 tqdm==4.66.4 transformers==4.41.0 umap==0.1.1 umap-learn==0.5.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22f35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from umap import UMAP\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='error_log.txt', level=logging.ERROR)\n",
    "\n",
    "def cluster_embeddings(embeddings, method='kmeans', n_clusters=2):\n",
    "    try:\n",
    "        if method == 'kmeans':\n",
    "            clusterer = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "        elif method == 'agglomerative':\n",
    "            clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid clustering method\")\n",
    "        return clusterer.fit_predict(embeddings), clusterer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in clustering: {str(e)}\")\n",
    "        return np.array([]), None\n",
    "\n",
    "def flip_labels(predicted_labels, label1, label2):\n",
    "    flipped_labels = np.where(predicted_labels == label1, label2, predicted_labels)\n",
    "    flipped_labels = np.where(predicted_labels == label2, label1, flipped_labels)\n",
    "    return flipped_labels\n",
    "\n",
    "def get_word_embeddings(sentences, words, true_labels, tokenizer, model):\n",
    "    try:\n",
    "        inputs = tokenizer(sentences, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        word_embeddings = []\n",
    "        valid_sentences = []\n",
    "        valid_words = []\n",
    "        valid_true_labels = []\n",
    "\n",
    "        for idx, (sentence, word, label) in enumerate(zip(sentences, words, true_labels)):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            word_token_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "            word_positions = []\n",
    "\n",
    "            for i in range(len(inputs.input_ids[idx]) - len(word_token_ids) + 1):\n",
    "                if inputs.input_ids[idx][i:i + len(word_token_ids)].tolist() == word_token_ids:\n",
    "                    word_positions.extend(range(i, i + len(word_token_ids)))\n",
    "\n",
    "            if word_positions:\n",
    "                word_embedding = token_embeddings[idx, word_positions, :].mean(dim=0).detach().cpu().numpy()\n",
    "                word_embeddings.append(word_embedding)\n",
    "                valid_sentences.append(sentence)\n",
    "                valid_words.append(word)\n",
    "                valid_true_labels.append(label)\n",
    "\n",
    "        return np.array(word_embeddings), valid_sentences, valid_words, valid_true_labels\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing sentences: {str(e)}\")\n",
    "        return np.array([]), [], [], []\n",
    "\n",
    "def evaluate_clustering(true_labels, predicted_labels):\n",
    "    try:\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "        return accuracy, f1\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error evaluating clustering: {str(e)}\")\n",
    "        return 0, 0\n",
    "\n",
    "def dimensionality_reduction(embeddings, method='pca', n_components=2):\n",
    "    try:\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "        elif method == 'mds':\n",
    "            reducer = MDS(n_components=n_components)\n",
    "        elif method == 'umap':\n",
    "            reducer = UMAP(n_components=n_components)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dimensionality reduction method\")\n",
    "        return reducer.fit_transform(embeddings)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in dimensionality reduction: {str(e)}\")\n",
    "        return np.array([])\n",
    "\n",
    "def process_word(word, df, model_name, cluster_method):\n",
    "    sentences = df[df['Word'] == word]['Sentence'].tolist()\n",
    "    true_labels_column = 'Sense' if 'Sense' in df.columns else 'Sense'\n",
    "    true_labels = df[df['Word'] == word][true_labels_column].tolist()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    embeddings, valid_sentences, valid_words, valid_true_labels = get_word_embeddings(\n",
    "        sentences, df[df['Word'] == word]['Word'].tolist(), true_labels, tokenizer, model\n",
    "    )\n",
    "\n",
    "    if embeddings.size == 0:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    predicted_labels, _ = cluster_embeddings(embeddings, method=cluster_method, n_clusters=len(set(valid_true_labels)))\n",
    "    if predicted_labels.size == 0:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    flipped_labels = flip_labels(predicted_labels, 0, 1)\n",
    "    accuracy = max(*evaluate_clustering(valid_true_labels, predicted_labels),\n",
    "                   *evaluate_clustering(valid_true_labels, flipped_labels))\n",
    "    f1 = max(*evaluate_clustering(valid_true_labels, predicted_labels),\n",
    "             *evaluate_clustering(valid_true_labels, flipped_labels))\n",
    "    misclassified = np.where(np.array(valid_true_labels) != predicted_labels)[0]\n",
    "\n",
    "    return embeddings, predicted_labels, valid_sentences, f1, accuracy, misclassified\n",
    "\n",
    "\n",
    "def create_plot_grid(df, model_names, cluster_methods, reduce_methods, output_dir='/Users/syamimmanuelpaulbondada/Downloads/html_report'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    report_html = \"\"\"<html>\n",
    "<head>\n",
    "<style>\n",
    "    body { font-family: Arial, sans-serif; }\n",
    "    table { width: 50%; border-collapse: collapse; margin-bottom: 20px; }\n",
    "    th, td { border: 1px solid black; padding: 8px; text-align: left; }\n",
    "    th { background-color: #f2f2f2; }\n",
    "    ul { list-style-type: none; padding: 0; }\n",
    "    ul li { margin: 5px 0; }\n",
    "</style>\n",
    "<script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "</head>\n",
    "<body>\"\"\"\n",
    "    overall_results = {model: {method: {'f1_scores': [], 'accuracies': []} for method in cluster_methods} for model in model_names}\n",
    "\n",
    "    for word in df['Word'].unique():\n",
    "        report_html += f\"<h2>Word: {word}</h2>\"\n",
    "\n",
    "        for cluster_method in cluster_methods:\n",
    "            report_html += f\"<h3>Clustering Method: {cluster_method}</h3>\"\n",
    "            report_html += \"<table><tr><th>Model</th><th>Accuracy</th><th>F1 Score</th></tr>\"\n",
    "\n",
    "            fig = make_subplots(\n",
    "                rows=len(model_names),\n",
    "                cols=len(reduce_methods),\n",
    "                subplot_titles=[f\"{model.split('/')[-1]} - {reduce}\" for model in model_names for reduce in reduce_methods],\n",
    "                vertical_spacing=0.1,\n",
    "                horizontal_spacing=0.05\n",
    "            )\n",
    "\n",
    "            for i, model_name in enumerate(model_names):\n",
    "                # Unpack the 6 values returned by the process_word function\n",
    "                embeddings, labels, sentences, f1, accuracy, misclassified = process_word(word, df, model_name, cluster_method)\n",
    "\n",
    "                if embeddings is None:\n",
    "                    continue\n",
    "\n",
    "                if f1 is not None and accuracy is not None:\n",
    "                    overall_results[model_name][cluster_method]['f1_scores'].append(f1)\n",
    "                    overall_results[model_name][cluster_method]['accuracies'].append(accuracy)\n",
    "\n",
    "\n",
    "                reduced_embeddings_dict = {reduce_method: dimensionality_reduction(embeddings, method=reduce_method) for reduce_method in reduce_methods}\n",
    "\n",
    "                hover_texts = [f\"Sentence: {sent}\" for sent in sentences]\n",
    "\n",
    "                for j, reduce_method in enumerate(reduce_methods):\n",
    "                    reduced_embeddings = reduced_embeddings_dict[reduce_method]\n",
    "                    if reduced_embeddings.size == 0:\n",
    "                        continue\n",
    "\n",
    "                    scatter = go.Scatter(\n",
    "                        x=reduced_embeddings[:, 0],\n",
    "                        y=reduced_embeddings[:, 1],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            color=labels,\n",
    "                            colorscale='Viridis',\n",
    "                            size=10\n",
    "                        ),\n",
    "                        text=hover_texts,\n",
    "                        hoverinfo='text',\n",
    "                        showlegend=False\n",
    "                    )\n",
    "\n",
    "                    fig.add_trace(scatter, row=i + 1, col=j + 1)\n",
    "\n",
    "                accuracy_str = f\"{accuracy:.2f}\" if accuracy is not None else \"N/A\"\n",
    "                f1_str = f\"{f1:.2f}\" if f1 is not None else \"N/A\"\n",
    "                report_html += f\"<tr><td>{model_name}</td><td>{accuracy_str}</td><td>{f1_str}</td></tr>\"\n",
    "\n",
    "            plot_height_per_row = 700  # Increased height for each row\n",
    "            plot_width_per_col = 700  # Increased width for each column\n",
    "            total_height = plot_height_per_row * len(model_names)\n",
    "            total_width = plot_width_per_col * len(reduce_methods)\n",
    "            \n",
    "            fig.update_layout(\n",
    "                height=total_height,\n",
    "                width=total_width,\n",
    "                title_text=f\"Word: {word} - Clustering: {cluster_method}\",\n",
    "            )\n",
    "\n",
    "            plot_html = pio.to_html(fig, full_html=False)\n",
    "            report_html += plot_html\n",
    "            report_html += \"</table>\"\n",
    "\n",
    "    report_html += \"<h2>Overall model evaluation</h2>\"\n",
    "    report_html += \"<table><tr><th>Model</th><th>Clustering Method</th><th>Average Accuracy</th><th>Average F1 Score</th></tr>\"\n",
    "    for model_name, cluster_results in overall_results.items():\n",
    "        for cluster_method, scores in cluster_results.items():\n",
    "            avg_accuracy = np.mean(scores['accuracies']) if scores['accuracies'] else 0\n",
    "            avg_f1 = np.mean(scores['f1_scores']) if scores['f1_scores'] else 0\n",
    "            report_html += f\"<tr><td>{model_name}</td><td>{cluster_method}</td><td>{avg_accuracy:.2f}</td><td>{avg_f1:.2f}</td></tr>\"\n",
    "    report_html += \"</table>\"\n",
    "\n",
    "    report_html += \"\"\"\n",
    "    <script>\n",
    "        document.addEventListener('DOMContentLoaded', function() {\n",
    "            function setPlotHeight(height) {\n",
    "                var plotDivs = document.querySelectorAll('.plotly-graph-div');\n",
    "                plotDivs.forEach(function(div) {\n",
    "                    div.style.height = height + 'px';\n",
    "                });\n",
    "            }\n",
    "            var plotHeightPerRow = 700;  // Increased to match the new height\n",
    "            var totalHeight = plotHeightPerRow * \"\"\" + str(len(model_names)) + \"\"\";\n",
    "            setPlotHeight(totalHeight);\n",
    "            var plots = document.querySelectorAll('[id^=\"plotly\"]');\n",
    "            plots.forEach(function(plot) {\n",
    "                Plotly.relayout(plot, { height: totalHeight });\n",
    "            });\n",
    "        });\n",
    "    </script>\n",
    "    </body>\n",
    "    </html>\"\"\"\n",
    "\n",
    "    with open(os.path.join(output_dir, 'Accuracies_and_plots.html'), 'w', encoding='utf-8') as f:\n",
    "        f.write(report_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb81a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at l3cube-pune/telugu-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google/muril-large-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "csv_path = '/Users/syamimmanuelpaulbondada/Downloads/kooli_words.csv'  \n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "model_names = [\n",
    "    'pierluigic/xl-lexeme',\n",
    "    'google/muril-base-cased',\n",
    "    'l3cube-pune/telugu-bert',\n",
    "    'google/muril-large-cased'\n",
    "]\n",
    "cluster_methods = ['kmeans']\n",
    "reduce_methods = ['umap']\n",
    "\n",
    "create_plot_grid(df, model_names, cluster_methods, reduce_methods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
